import streamlit as st
import boto3
from openai import OpenAI
import google.generativeai as genai
import time

# =========================================================
# üî¥ 1. ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API KEYS (‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà)
# =========================================================
AWS_ACCESS_KEY = "AKIAxxxxxxxxxxxxxxxx"       
AWS_SECRET_KEY = "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" 
KB_ID = "XXXXXXXXXX"  
REGION = "us-east-1"

DEEPSEEK_API_KEY = "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" 
GEMINI_API_KEY = "AIzaSyxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# =========================================================
# ‚öôÔ∏è 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö (Page Config)
# =========================================================
st.set_page_config(
    page_title="‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö AI - ‡∏®‡∏≤‡∏•‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =========================================================
# üé® 3. ‡∏ï‡∏Å‡πÅ‡∏ï‡πà‡∏á CSS (‡∏ò‡∏µ‡∏°‡∏®‡∏≤‡∏•‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á: ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô/‡∏ó‡∏≠‡∏á/‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏™‡∏≤‡∏£‡∏ö‡∏£‡∏£‡∏ì)
# =========================================================
st.markdown("""
<style>
    /* ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏ü‡∏≠‡∏ô‡∏ï‡πå Sarabun ‡∏à‡∏≤‡∏Å Google Fonts */
    @import url('https://fonts.googleapis.com/css2?family=Sarabun:wght@300;400;500;700&display=swap');

    /* ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö */
    html, body, [class*="css"] {
        font-family: 'Sarabun', sans-serif;
    }

    /* ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô */
    .main-header {
        background-color: #002D62; /* ‡∏™‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏Ç‡πâ‡∏°‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£ */
        padding: 20px;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 20px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .main-header h1 {
        color: #FFD700; /* ‡∏™‡∏µ‡∏ó‡∏≠‡∏á */
        font-weight: 700;
        margin: 0;
        font-size: 28px;
    }
    .main-header p {
        color: #E0E0E0;
        margin-top: 5px;
        font-size: 16px;
    }

    /* ‡∏Å‡∏≤‡∏£‡πå‡∏î‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå */
    .result-card {
        background-color: white;
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        padding: 20px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        height: 100%;
    }
    .model-badge {
        display: inline-block;
        padding: 4px 12px;
        border-radius: 20px;
        font-size: 14px;
        font-weight: bold;
        color: white;
        margin-bottom: 15px;
    }
    .badge-aws { background-color: #232F3E; } /* ‡∏™‡∏µ AWS */
    .badge-deepseek { background-color: #4B0082; } /* ‡∏™‡∏µ‡∏°‡πà‡∏ß‡∏á */
    .badge-gemini { background-color: #1E88E5; } /* ‡∏™‡∏µ‡∏ü‡πâ‡∏≤ Google */

    /* ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Sidebar */
    [data-testid="stSidebar"] {
        background-color: #F8F9FA;
        border-right: 1px solid #ddd;
    }
    
    /* ‡∏Å‡∏•‡πà‡∏≠‡∏á Source */
    .source-box {
        background-color: #F0F4F8;
        border-left: 4px solid #002D62;
        padding: 10px;
        font-size: 14px;
        margin-top: 10px;
        border-radius: 4px;
    }
</style>
""", unsafe_allow_html=True)

# =========================================================
# üîß 4. Setup Clients & Logic
# =========================================================

# --- Clients Setup ---
@st.cache_resource
def get_aws_client():
    return boto3.client(
        service_name='bedrock-agent-runtime',
        region_name=REGION,
        aws_access_key_id=AWS_ACCESS_KEY,
        aws_secret_access_key=AWS_SECRET_KEY
    )

@st.cache_resource
def get_deepseek_client():
    return OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com")

genai.configure(api_key=GEMINI_API_KEY)

# --- Models List ---
MODELS = {
    "Claude 3.5 Sonnet (AWS)": {"type": "bedrock", "id": "arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0", "color": "badge-aws"},
    "Claude 3 Haiku (AWS)": {"type": "bedrock", "id": "arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0", "color": "badge-aws"},
    "DeepSeek V3 (API)": {"type": "deepseek", "id": "deepseek-chat", "color": "badge-deepseek"},
    "Gemini 1.5 Flash (Google)": {"type": "gemini", "id": "gemini-1.5-flash", "color": "badge-gemini"},
    "Gemini 1.5 Pro (Google)": {"type": "gemini", "id": "gemini-1.5-pro", "color": "badge-gemini"},
}

# --- Shared Functions ---
def get_retrieved_context(prompt):
    aws_client = get_aws_client()
    try:
        retrieval = aws_client.retrieve(
            knowledgeBaseId=KB_ID,
            retrievalQuery={'text': prompt},
            retrievalConfiguration={'vectorSearchConfiguration': {'numberOfResults': 5}}
        )
        context_text = ""
        citations = []
        if 'retrievalResults' in retrieval:
            for result in retrieval['retrievalResults']:
                text = result['content']['text']
                uri = result['location']['s3Location']['uri']
                context_text += f"- {text}\n"
                citations.append({
                    'retrievedReferences': [{
                        'content': {'text': text},
                        'location': {'s3Location': {'uri': uri}}
                    }]
                })
        return context_text, citations
    except Exception as e:
        return None, str(e)

# --- AI Functions ---
def ask_bedrock(prompt, model_arn):
    client = get_aws_client()
    try:
        response = client.retrieve_and_generate(
            input={'text': prompt},
            retrieveAndGenerateConfiguration={
                'type': 'KNOWLEDGE_BASE',
                'knowledgeBaseConfiguration': {
                    'knowledgeBaseId': KB_ID,
                    'modelArn': model_arn
                }
            }
        )
        return response['output']['text'], response.get('citations', [])
    except Exception as e:
        return f"Error: {str(e)}", []

def ask_deepseek(prompt, model_name):
    ds_client = get_deepseek_client()
    context, cites = get_retrieved_context(prompt)
    if context is None: return f"Search Error: {cites}", []
    if not context: return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£", []
    
    try:
        res = ds_client.chat.completions.create(
            model=model_name,
            messages=[{"role": "system", "content": f"‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ: {context}"}, {"role": "user", "content": prompt}],
            stream=False
        )
        return res.choices[0].message.content, cites
    except Exception as e:
        if "402" in str(e): return "‚ö†Ô∏è DeepSeek Credit ‡∏´‡∏°‡∏î (Error 402)", cites
        return f"Error: {str(e)}", []

def ask_gemini(prompt, model_name):
    context, cites = get_retrieved_context(prompt)
    if context is None: return f"Search Error: {cites}", []
    if not context: return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£", []

    try:
        model = genai.GenerativeModel(model_name)
        res = model.generate_content(f"Context: {context}\n\nQuestion: {prompt}")
        return res.text, cites
    except Exception as e:
        return f"Error: {str(e)}", []

def query_router(prompt, model_key):
    config = MODELS[model_key]
    if config["type"] == "bedrock": return ask_bedrock(prompt, config["id"])
    elif config["type"] == "deepseek": return ask_deepseek(prompt, config["id"])
    elif config["type"] == "gemini": return ask_gemini(prompt, config["id"])

# =========================================================
# üñ•Ô∏è 5. ‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• (UI Layout)
# =========================================================

# --- Sidebar ---
with st.sidebar:
    st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Emblem_of_the_Administrative_Court_of_Thailand.svg/200px-Emblem_of_the_Administrative_Court_of_Thailand.svg.png", width=100) # Placeholder Logo
    st.markdown("### ‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
    st.info("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏∏‡πà‡∏ô‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå")
    
    model_left = st.selectbox("ü§ñ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ù‡∏±‡πà‡∏á‡∏ã‡πâ‡∏≤‡∏¢", list(MODELS.keys()), index=0)
    model_right = st.selectbox("ü¶Å ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ù‡∏±‡πà‡∏á‡∏Ç‡∏ß‡∏≤", list(MODELS.keys()), index=3)
    
    st.markdown("---")
    if st.button("‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤", type="primary"):
        st.session_state.history = []
        st.rerun()

# --- Main Content ---
# Header ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°
st.markdown("""
<div class="main-header">
    <h1>‚öñÔ∏è ‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏®‡∏≤‡∏•‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á</h1>
    <p>AI-Powered Knowledge Retrieval & Comparison System</p>
</div>
""", unsafe_allow_html=True)

# Session State
if "history" not in st.session_state: st.session_state.history = []

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
for chat in st.session_state.history:
    st.markdown(f"#### üó£Ô∏è ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {chat['question']}")
    
    col1, col2 = st.columns(2)
    
    # Card ‡∏ù‡∏±‡πà‡∏á‡∏ã‡πâ‡∏≤‡∏¢
    with col1:
        config = MODELS[chat['m1']]
        st.markdown(f"""
        <div class="result-card">
            <div class="model-badge {config['color']}">{chat['m1']}</div>
            <div style="line-height: 1.6; color: #333;">{chat['a1']}</div>
        </div>
        """, unsafe_allow_html=True)
        
        # Source (Separate Expander for cleaner look)
        if chat['c1']:
            with st.expander("üìÑ ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (Source)"):
                seen = set()
                for c in chat['c1']:
                    uri = c['retrievedReferences'][0]['location']['s3Location']['uri'].split('/')[-1]
                    if uri not in seen:
                        st.markdown(f"<div class='source-box'>üìé {uri}</div>", unsafe_allow_html=True)
                        seen.add(uri)

    # Card ‡∏ù‡∏±‡πà‡∏á‡∏Ç‡∏ß‡∏≤
    with col2:
        config = MODELS[chat['m2']]
        st.markdown(f"""
        <div class="result-card">
            <div class="model-badge {config['color']}">{chat['m2']}</div>
            <div style="line-height: 1.6; color: #333;">{chat['a2']}</div>
        </div>
        """, unsafe_allow_html=True)
        
        if chat['c2']:
            with st.expander("üìÑ ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á (Source)"):
                seen = set()
                for c in chat['c2']:
                    uri = c['retrievedReferences'][0]['location']['s3Location']['uri'].split('/')[-1]
                    if uri not in seen:
                        st.markdown(f"<div class='source-box'>üìé {uri}</div>", unsafe_allow_html=True)
                        seen.add(uri)
    
    st.markdown("---")

# --- Input Area (‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏î) ---
prompt = st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∑‡∏ö‡∏Ñ‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏û‡∏¥‡∏û‡∏≤‡∏Å‡∏©‡∏≤...")

if prompt:
    # Logic ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å AI (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏¥‡πà‡∏° Loading ‡∏™‡∏ß‡∏¢‡πÜ)
    with st.spinner("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏∑‡∏ö‡∏Ñ‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£..."):
        a1, c1 = query_router(prompt, model_left)
        a2, c2 = query_router(prompt, model_right)
    
    st.session_state.history.append({
        "question": prompt,
        "m1": model_left, "a1": a1, "c1": c1,
        "m2": model_right, "a2": a2, "c2": c2
    })
    st.rerun()
